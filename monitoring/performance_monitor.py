"""
PUBG ë°ì´í„° íŒŒì´í”„ë¼ì¸ ì„±ëŠ¥ ëª¨ë‹ˆí„°ë§ ì‹œìŠ¤í…œ
- Kafka ì²˜ë¦¬ëŸ‰ ë° ì§€ì—°ì‹œê°„ ëª¨ë‹ˆí„°ë§
- Spark ì„±ëŠ¥ ë©”íŠ¸ë¦­ ìˆ˜ì§‘
- ì‹œìŠ¤í…œ ë¦¬ì†ŒìŠ¤ ì‚¬ìš©ëŸ‰ ì¶”ì 
- ì‹¤ì‹œê°„ ì„±ëŠ¥ ëŒ€ì‹œë³´ë“œ ë°ì´í„° ì œê³µ
"""

import os
import time
import psutil
import logging
import pytz
from datetime import datetime
from kafka import KafkaConsumer, KafkaAdminClient
from kafka.structs import TopicPartition
from influxdb_client import InfluxDBClient, Point
from influxdb_client.client.write_api import SYNCHRONOUS
import requests
import json
from threading import Thread
import concurrent.futures

# í•œêµ­ ì‹œê°„ëŒ€ ì„¤ì •
KST = pytz.timezone('Asia/Seoul')

# ë¡œê¹… ì„¤ì •
logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')
logger = logging.getLogger(__name__)

# í™˜ê²½ ë³€ìˆ˜
KAFKA_BOOTSTRAP_SERVERS = os.getenv("KAFKA_BOOTSTRAP_SERVERS", "kafka:29092")
INFLUXDB_URL = os.getenv("INFLUXDB_URL", "http://influxdb:8086")
INFLUXDB_TOKEN = os.getenv("INFLUXDB_TOKEN", "my-super-secret-auth-token")
INFLUXDB_ORG = os.getenv("INFLUXDB_ORG", "pubg-org")
INFLUXDB_BUCKET = os.getenv("INFLUXDB_BUCKET", "pubg-metrics")

class PerformanceMonitor:
    def __init__(self):
        self.influx_client = None
        self.write_api = None
        self.kafka_admin = None
        self.running = True
        self.performance_data = {
            'kafka_lag': {},
            'processing_times': {},
            'system_resources': {}
        }
        
    def setup_clients(self):
        """í´ë¼ì´ì–¸íŠ¸ ì—°ê²° ì„¤ì •"""
        try:
            # InfluxDB í´ë¼ì´ì–¸íŠ¸
            self.influx_client = InfluxDBClient(
                url=INFLUXDB_URL,
                token=INFLUXDB_TOKEN,
                org=INFLUXDB_ORG
            )
            self.write_api = self.influx_client.write_api(write_options=SYNCHRONOUS)
            logger.info("âœ… InfluxDB ì—°ê²° ì„±ê³µ")
            
            # Kafka Admin í´ë¼ì´ì–¸íŠ¸
            self.kafka_admin = KafkaAdminClient(
                bootstrap_servers=KAFKA_BOOTSTRAP_SERVERS,
                client_id="performance-monitor"
            )
            logger.info("âœ… Kafka Admin ì—°ê²° ì„±ê³µ")
            
        except Exception as e:
            logger.error(f"âŒ í´ë¼ì´ì–¸íŠ¸ ì—°ê²° ì‹¤íŒ¨: {e}")
            
    def collect_kafka_metrics(self):
        """Kafka ì„±ëŠ¥ ë©”íŠ¸ë¦­ ìˆ˜ì§‘"""
        try:
            topics = ['pubg-matches', 'pubg-alerts', 'pubg-aggregates']
            
            for topic in topics:
                # ì»¨ìŠˆë¨¸ ê·¸ë£¹ë³„ ì§€ì—°ì‹œê°„ í™•ì¸
                consumer = KafkaConsumer(
                    topic,
                    bootstrap_servers=KAFKA_BOOTSTRAP_SERVERS,
                    group_id=f'performance-monitor-{topic}',
                    auto_offset_reset='latest',
                    enable_auto_commit=False,
                    consumer_timeout_ms=5000
                )
                
                # íŒŒí‹°ì…˜ë³„ ë©”íŠ¸ë¦­
                partitions = consumer.partitions_for_topic(topic)
                if partitions:
                    for partition in partitions:
                        tp = TopicPartition(topic, partition)
                        
                        # í˜„ì¬ ì˜¤í”„ì…‹ê³¼ ìµœì‹  ì˜¤í”„ì…‹ ë¹„êµ
                        committed = consumer.committed(tp)
                        end_offsets = consumer.end_offsets([tp])
                        
                        if committed is not None and tp in end_offsets:
                            lag = end_offsets[tp] - committed
                            
                            # InfluxDBì— ì €ì¥
                            point = Point("kafka_performance") \
                                .tag("topic", topic) \
                                .tag("partition", str(partition)) \
                                .field("consumer_lag", lag) \
                                .field("latest_offset", end_offsets[tp]) \
                                .field("committed_offset", committed) \
                                .time(datetime.now(KST))
                            
                            self.write_api.write(bucket=INFLUXDB_BUCKET, record=point)
                            
                            # ë©”ëª¨ë¦¬ì—ë„ ì €ì¥
                            key = f"{topic}_p{partition}"
                            self.performance_data['kafka_lag'][key] = {
                                'lag': lag,
                                'latest_offset': end_offsets[tp],
                                'committed_offset': committed,
                                'timestamp': time.time()
                            }
                
                consumer.close()
                
        except Exception as e:
            logger.error(f"âŒ Kafka ë©”íŠ¸ë¦­ ìˆ˜ì§‘ ì‹¤íŒ¨: {e}")
    
    def collect_spark_metrics(self):
        """Spark ì„±ëŠ¥ ë©”íŠ¸ë¦­ ìˆ˜ì§‘"""
        try:
            # Spark Master Web UIì—ì„œ ë©”íŠ¸ë¦­ ìˆ˜ì§‘
            spark_master_url = "http://spark-master:8080/json/"
            
            response = requests.get(spark_master_url, timeout=10)
            if response.status_code == 200:
                spark_data = response.json()
                
                # Active ì• í”Œë¦¬ì¼€ì´ì…˜ ë©”íŠ¸ë¦­
                for app in spark_data.get('activeapps', []):
                    app_id = app.get('id', 'unknown')
                    
                    point = Point("spark_performance") \
                        .tag("application_id", app_id) \
                        .tag("application_name", app.get('name', 'unknown')) \
                        .field("cores_granted", app.get('coresgranted', 0)) \
                        .field("cores_requested", app.get('coresrequested', 0)) \
                        .field("memory_used", app.get('memoryused', 0)) \
                        .field("executors", len(app.get('executorinfos', []))) \
                        .field("duration", app.get('duration', 0)) \
                        .time(datetime.now(KST))
                    
                    self.write_api.write(bucket=INFLUXDB_BUCKET, record=point)
                
                # ì›Œì»¤ ë©”íŠ¸ë¦­
                for worker in spark_data.get('workers', []):
                    point = Point("spark_workers") \
                        .tag("worker_id", worker.get('id', 'unknown')) \
                        .tag("worker_host", worker.get('host', 'unknown')) \
                        .field("cores_used", worker.get('coresused', 0)) \
                        .field("cores_free", worker.get('coresfree', 0)) \
                        .field("memory_used", worker.get('memoryused', 0)) \
                        .field("memory_free", worker.get('memoryfree', 0)) \
                        .field("active_drivers", worker.get('activedrivers', 0)) \
                        .field("active_executors", worker.get('activeexecutors', 0)) \
                        .time(datetime.now(KST))
                    
                    self.write_api.write(bucket=INFLUXDB_BUCKET, record=point)
                    
        except Exception as e:
            logger.error(f"âŒ Spark ë©”íŠ¸ë¦­ ìˆ˜ì§‘ ì‹¤íŒ¨: {e}")
    
    def collect_system_metrics(self):
        """ì‹œìŠ¤í…œ ë¦¬ì†ŒìŠ¤ ë©”íŠ¸ë¦­ ìˆ˜ì§‘"""
        try:
            # CPU ì‚¬ìš©ë¥ 
            cpu_percent = psutil.cpu_percent(interval=1)
            
            # ë©”ëª¨ë¦¬ ì‚¬ìš©ë¥ 
            memory = psutil.virtual_memory()
            
            # ë””ìŠ¤í¬ ì‚¬ìš©ë¥ 
            disk = psutil.disk_usage('/')
            
            # ë„¤íŠ¸ì›Œí¬ I/O
            net_io = psutil.net_io_counters()
            
            # ì‹œìŠ¤í…œ ë©”íŠ¸ë¦­ ì €ì¥
            point = Point("system_performance") \
                .field("cpu_percent", cpu_percent) \
                .field("memory_percent", memory.percent) \
                .field("memory_available", memory.available) \
                .field("memory_total", memory.total) \
                .field("disk_percent", (disk.used / disk.total) * 100) \
                .field("disk_free", disk.free) \
                .field("disk_total", disk.total) \
                .field("network_bytes_sent", net_io.bytes_sent) \
                .field("network_bytes_recv", net_io.bytes_recv) \
                .field("network_packets_sent", net_io.packets_sent) \
                .field("network_packets_recv", net_io.packets_recv) \
                .time(datetime.now(KST))
            
            self.write_api.write(bucket=INFLUXDB_BUCKET, record=point)
            
            # ë©”ëª¨ë¦¬ì—ë„ ì €ì¥
            self.performance_data['system_resources'] = {
                'cpu_percent': cpu_percent,
                'memory_percent': memory.percent,
                'disk_percent': (disk.used / disk.total) * 100,
                'timestamp': time.time()
            }
            
        except Exception as e:
            logger.error(f"âŒ ì‹œìŠ¤í…œ ë©”íŠ¸ë¦­ ìˆ˜ì§‘ ì‹¤íŒ¨: {e}")
    
    def collect_processing_latency(self):
        """ì²˜ë¦¬ ì§€ì—°ì‹œê°„ ë©”íŠ¸ë¦­ ìˆ˜ì§‘"""
        try:
            # InfluxDBì—ì„œ ìµœê·¼ ì²˜ë¦¬ ì‹œê°„ ë°ì´í„° ê°€ì ¸ì˜¤ê¸°
            query = f'''
            from(bucket: "{INFLUXDB_BUCKET}")
              |> range(start: -5m)
              |> filter(fn: (r) => r["_measurement"] == "player_performance")
              |> filter(fn: (r) => r["_field"] == "processing_latency")
              |> last()
            '''
            
            tables = self.influx_client.query_api().query(query, org=INFLUXDB_ORG)
            
            total_latency = 0
            count = 0
            
            for table in tables:
                for record in table.records:
                    total_latency += record.get_value()
                    count += 1
            
            if count > 0:
                avg_latency = total_latency / count
                
                point = Point("processing_performance") \
                    .field("avg_processing_latency", avg_latency) \
                    .field("total_processed", count) \
                    .time(datetime.now(KST))
                
                self.write_api.write(bucket=INFLUXDB_BUCKET, record=point)
                
        except Exception as e:
            logger.error(f"âŒ ì²˜ë¦¬ ì§€ì—°ì‹œê°„ ë©”íŠ¸ë¦­ ìˆ˜ì§‘ ì‹¤íŒ¨: {e}")
    
    def generate_performance_summary(self):
        """ì„±ëŠ¥ ìš”ì•½ ë¦¬í¬íŠ¸ ìƒì„±"""
        try:
            current_time = datetime.now(KST)
            
            # ì„±ëŠ¥ ìš”ì•½ ë°ì´í„°
            kafka_total_lag = sum([data['lag'] for data in self.performance_data['kafka_lag'].values()])
            
            system_data = self.performance_data.get('system_resources', {})
            
            summary_point = Point("performance_summary") \
                .field("kafka_total_lag", kafka_total_lag) \
                .field("kafka_topics_monitored", len(self.performance_data['kafka_lag'])) \
                .field("system_cpu_percent", system_data.get('cpu_percent', 0)) \
                .field("system_memory_percent", system_data.get('memory_percent', 0)) \
                .field("system_disk_percent", system_data.get('disk_percent', 0)) \
                .field("monitoring_health", 1) \
                .time(current_time)
            
            self.write_api.write(bucket=INFLUXDB_BUCKET, record=summary_point)
            
            logger.info(f"ğŸ“Š ì„±ëŠ¥ ìš”ì•½: Kafka ì§€ì—° {kafka_total_lag}ê±´, CPU {system_data.get('cpu_percent', 0):.1f}%, ë©”ëª¨ë¦¬ {system_data.get('memory_percent', 0):.1f}%")
            
        except Exception as e:
            logger.error(f"âŒ ì„±ëŠ¥ ìš”ì•½ ìƒì„± ì‹¤íŒ¨: {e}")
    
    def run_monitoring_cycle(self):
        """ëª¨ë‹ˆí„°ë§ ì‚¬ì´í´ ì‹¤í–‰"""
        logger.info("ğŸš€ ì„±ëŠ¥ ëª¨ë‹ˆí„°ë§ ì‹œì‘")
        
        while self.running:
            try:
                start_time = time.time()
                
                # ë³‘ë ¬ë¡œ ë©”íŠ¸ë¦­ ìˆ˜ì§‘
                with concurrent.futures.ThreadPoolExecutor(max_workers=4) as executor:
                    futures = [
                        executor.submit(self.collect_kafka_metrics),
                        executor.submit(self.collect_spark_metrics),
                        executor.submit(self.collect_system_metrics),
                        executor.submit(self.collect_processing_latency)
                    ]
                    
                    # ëª¨ë“  ì‘ì—… ì™„ë£Œ ëŒ€ê¸°
                    concurrent.futures.wait(futures, timeout=30)
                
                # ì„±ëŠ¥ ìš”ì•½ ìƒì„±
                self.generate_performance_summary()
                
                cycle_time = time.time() - start_time
                logger.info(f"â±ï¸ ëª¨ë‹ˆí„°ë§ ì‚¬ì´í´ ì™„ë£Œ: {cycle_time:.2f}ì´ˆ")
                
                # 30ì´ˆë§ˆë‹¤ ì‹¤í–‰
                time.sleep(30)
                
            except Exception as e:
                logger.error(f"âŒ ëª¨ë‹ˆí„°ë§ ì‚¬ì´í´ ì˜¤ë¥˜: {e}")
                time.sleep(10)
    
    def shutdown(self):
        """ëª¨ë‹ˆí„°ë§ ì¢…ë£Œ"""
        self.running = False
        if self.influx_client:
            self.influx_client.close()
        logger.info("ğŸ›‘ ì„±ëŠ¥ ëª¨ë‹ˆí„°ë§ ì¢…ë£Œ")

def main():
    monitor = PerformanceMonitor()
    
    try:
        monitor.setup_clients()
        monitor.run_monitoring_cycle()
    except KeyboardInterrupt:
        logger.info("ì‚¬ìš©ìì— ì˜í•œ ì¢…ë£Œ")
    except Exception as e:
        logger.error(f"âŒ ëª¨ë‹ˆí„°ë§ ì‹¤í–‰ ì‹¤íŒ¨: {e}")
    finally:
        monitor.shutdown()

if __name__ == "__main__":
    main()